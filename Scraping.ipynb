{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a885c-1230-4698-9023-15afffc13f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, ElementNotInteractableException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def scrape_restaurant(driver, url):\n",
    "    global rest_count\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        rest_name = driver.find_element(By.XPATH, '//h1').text\n",
    "    except NoSuchElementException:\n",
    "        rest_name = \"Name not found\"\n",
    "    \n",
    "    try:\n",
    "        number_of_reviews = driver.find_element(By.XPATH, '//*[@id=\"reviewInfo\"]/span[2]').text\n",
    "    except NoSuchElementException:\n",
    "        number_of_reviews = \"Reviews not found\"\n",
    "    \n",
    "    try:\n",
    "        rating = driver.find_element(By.XPATH, './/span[contains(@class, \"m1KNa9XKCHY- C7Tp-bANpE4-\")]').text\n",
    "    except NoSuchElementException:\n",
    "        rating = \"Rating not found\"\n",
    "    \n",
    "    try:\n",
    "        food_type = driver.find_element(By.XPATH, '//*[@id=\"cuisineInfo\"]/span[2]').text\n",
    "    except NoSuchElementException:\n",
    "        food_type = \"Food type not found\"\n",
    "    \n",
    "    try:\n",
    "        coupon = driver.find_element(By.XPATH, '//div[contains(@id, \"priceBandInfo\")]//span[last()]').text\n",
    "    except NoSuchElementException:\n",
    "        coupon = \"Coupon not found\"\n",
    "\n",
    "    try:\n",
    "        food = driver.find_element(By.XPATH, '//span[text()=\"Food\"]/preceding-sibling::span').text\n",
    "    except NoSuchElementException:\n",
    "        food = \"Food not found\"\n",
    "\n",
    "    try:\n",
    "        service = driver.find_element(By.XPATH, '//span[text()=\"Service\"]/preceding-sibling::span').text\n",
    "    except NoSuchElementException:\n",
    "        service = \"Service not found\"\n",
    "\n",
    "    try:\n",
    "        ambience = driver.find_element(By.XPATH, '//span[text()=\"Ambience\"]/preceding-sibling::span').text\n",
    "    except NoSuchElementException:\n",
    "        ambience = \"Ambience not found\"\n",
    "\n",
    "    try:\n",
    "        value = driver.find_element(By.XPATH, '//span[text()=\"Value\"]/preceding-sibling::span').text\n",
    "    except NoSuchElementException:\n",
    "        value = \"Value not found\"\n",
    "        \n",
    "    # Scraping image URL (assumed the second image is the restaurant's image)\n",
    "    try:\n",
    "        image_elements = driver.find_elements(By.XPATH, '//img[contains(@src, \"otstatic.com\")]')\n",
    "        if len(image_elements) > 1:  # Skip logo and take the second image\n",
    "            second_image_url = image_elements[1].get_attribute('src')\n",
    "        else:\n",
    "            second_image_url = \"No valid image found\"\n",
    "    except NoSuchElementException:\n",
    "        second_image_url = \"No image found\"\n",
    "    \n",
    "    # Initialize an empty list to store all comments\n",
    "    all_comments = []\n",
    "    max_pages = 3\n",
    "    current_page = 1\n",
    "    \n",
    "    # Scrape comments from 3 pages\n",
    "    while current_page <= max_pages:\n",
    "        try:\n",
    "            # Extract comments on the current page\n",
    "            comments_elements = driver.find_elements(By.XPATH, './/span[contains(@class, \"l9bbXUdC9v0- ZatlKKd1hyc- ukvN6yaH1Ds-\")]')\n",
    "            comments = \" \".join([element.text for element in comments_elements[1:]]) if comments_elements else \"Comments not found\"\n",
    "            all_comments.append(comments)\n",
    "            print(f\"Page {current_page} Comments:\", comments)  # Debug print\n",
    "            \n",
    "            # Look for the \"Next\" button to navigate to the next page of comments\n",
    "            try:\n",
    "                # Re-locate the \"Next\" button on each iteration to avoid StaleElementReferenceException\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, '//a[@aria-label=\"Go to the next page\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "                time.sleep(1)\n",
    "                driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                time.sleep(2)\n",
    "                current_page += 1\n",
    "            except (TimeoutException, ElementNotInteractableException, StaleElementReferenceException):\n",
    "                print(\"Failed to locate or click the next button. Ending pagination.\")\n",
    "                break  # If the next button is not clickable or not found, break the loop\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "    \n",
    "    # Join all comments into a single string\n",
    "    all_comments = \" \".join(all_comments)\n",
    "\n",
    "    try:\n",
    "        about_rest = driver.find_element(By.XPATH, './/span[contains(@class, \"l9bbXUdC9v0- ZatlKKd1hyc- ukvN6yaH1Ds- l-AMWW5ZrIg-\")]').text\n",
    "    except NoSuchElementException:\n",
    "        about_rest = \"About Restaurant not found\"\n",
    "    print(\"About Restaurant:\", about_rest)  # Debug print\n",
    "\n",
    "    # Append scraped data to the initialized dictionary\n",
    "    data[\"url\"].append(url)\n",
    "    data[\"rest_name\"].append(rest_name)\n",
    "    data[\"number_of_reviews\"].append(number_of_reviews)\n",
    "    data[\"rating\"].append(rating)\n",
    "    data[\"food_type\"].append(food_type)\n",
    "    data[\"coupon\"].append(coupon)\n",
    "    data[\"food\"].append(food)\n",
    "    data[\"service\"].append(service)\n",
    "    data[\"ambience\"].append(ambience)\n",
    "    data[\"value\"].append(value)\n",
    "    data[\"about_rest\"].append(about_rest)\n",
    "    data[\"comments\"].append(all_comments)\n",
    "    data[\"image_url\"].append(second_image_url)  # Add image URL to the data\n",
    "\n",
    "    rest_count += 1\n",
    "    print(f\"Total Restaurants scraped: {rest_count}\\n\")\n",
    "\n",
    "\n",
    "def get_restaurant_links(driver):\n",
    "    all_urls = []\n",
    "    time.sleep(random.randint(2, 3))  # Adding random delay to avoid detection\n",
    "\n",
    "    # Scroll increment\n",
    "    scroll_increment = 500  # Scroll down by 500 pixels at a time\n",
    "\n",
    "    # Perform multiple scrolls (controlled by max_scrolls)\n",
    "    for _ in range(22):\n",
    "        # Scroll down by the increment\n",
    "        driver.execute_script(f\"window.scrollBy(0, {scroll_increment});\")\n",
    "\n",
    "        # Wait for restaurant links to become visible and ready\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, \"//a[contains(@class, 'qCITanV81-Y-')]\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout waiting for restaurant links to become visible\")\n",
    "            break\n",
    "\n",
    "        # Find all restaurant link elements\n",
    "        elements = driver.find_elements(By.XPATH, \"//a[contains(@class, 'qCITanV81-Y-')]\")\n",
    "\n",
    "        # Add the href from each element to the list, avoiding duplicates\n",
    "        for elem in elements:\n",
    "            url = elem.get_attribute('href')\n",
    "            if url and url not in all_urls:\n",
    "                all_urls.append(url)\n",
    "        \n",
    "    print(f\"Total URLs collected so far: {len(all_urls)}\")\n",
    "    return all_urls\n",
    "\n",
    "\n",
    "def click_next_page(driver):\n",
    "    try:\n",
    "        # Locate the \"Next Page\" button by aria-label\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@aria-label='Go to the next page']\")\n",
    "        \n",
    "        # Scroll into view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "        \n",
    "        # Wait until the element is clickable\n",
    "        WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//a[@aria-label='Go to the next page']\")))\n",
    "        \n",
    "        # Perform click action\n",
    "        next_button.click()\n",
    "        return True\n",
    "    except (NoSuchElementException, ElementClickInterceptedException, ElementNotInteractableException, TimeoutException) as e:\n",
    "        print(f\"Could not click next page: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Data structure to store the restaurant details\n",
    "data = {\n",
    "    \"url\": [],\n",
    "    \"rest_name\": [],\n",
    "    \"number_of_reviews\": [],\n",
    "    \"rating\": [],\n",
    "    \"food_type\": [],\n",
    "    \"coupon\": [],\n",
    "    \"food\": [],\n",
    "    \"service\": [],\n",
    "    \"ambience\": [],\n",
    "    \"value\": [],\n",
    "    \"about_rest\": [],\n",
    "    \"comments\": [],\n",
    "    \"image_url\": []  # Added image_url to store the image links\n",
    "}\n",
    "\n",
    "# Set up the Selenium WebDriver (e.g., Chrome)\n",
    "search_url = 'https://www.opentable.com/s?dateTime=2024-09-28T19%3A00%3A00&covers=2&latitude=37.780885&longitude=-122.2852606&shouldUseLatLongSearch=false&originCorrelationId=f67565c9-ee78-45ce-863b-3b411800a93b'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(search_url)\n",
    "\n",
    "all_urls = []\n",
    "rest_count = 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Scrape restaurant URLs on the current page\n",
    "    urls = get_restaurant_links(driver)\n",
    "    all_urls.extend(urls)\n",
    "\n",
    "    # Try to go to the next page; if no next page, break the loop\n",
    "    if not click_next_page(driver):\n",
    "        print(\"No more pages to navigate.\")\n",
    "        break\n",
    "\n",
    "# Scrape data for each restaurant URL\n",
    "for url in all_urls:\n",
    "    scrape_restaurant(driver, url)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "pd.DataFrame(data).to_csv('restaurant_data_with_images.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
